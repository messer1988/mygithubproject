from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# === –ü—É—Ç—å –∫ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ===
model_path = "D:/ChatGPT_myModel/final_model"

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ===
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

# === –ó–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
while True:
    input_text = input("\nüîç –í–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit'): ")
    if input_text.lower() in ["exit", "quit", "–≤—ã—Ö–æ–¥"]:
        break

    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=128
    )

    # === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_length=128,
        num_return_sequences=1,
        do_sample=True,
        top_k=50,
        top_p=0.9,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id
    )

    # === –†–∞—Å–∫–æ–¥–∏—Ä–æ–≤–∫–∞
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print("\nü§ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\n", result)