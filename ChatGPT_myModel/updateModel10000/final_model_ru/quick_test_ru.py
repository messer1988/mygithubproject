from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from pathlib import Path

model_path = Path(__file__).resolve().parent
device = "mps" if torch.backends.mps.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(str(model_path))
model = AutoModelForCausalLM.from_pretrained(str(model_path)).to(device)
model.eval()

def ask(prompt):
    formatted = f"<|prompt|> {prompt}\n<|answer|>"
    inputs = tokenizer(formatted, return_tensors="pt").to(device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.8,
        top_p=0.95,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.convert_tokens_to_ids("<|prompt|>"),  # —Å—Ç–æ–ø –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –≤–æ–ø—Ä–æ—Å–µ
    )

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ‚úÇÔ∏è –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç
    if "<|answer|>" in text:
        text = text.split("<|answer|>")[-1]
    if "<|prompt|>" in text:
        text = text.split("<|prompt|>")[0]

    return text.strip()


while True:
    question = input("\n‚ùì –í–æ–ø—Ä–æ—Å: ")
    if question.lower() in ["exit", "quit", "q"]:
        break
    print("\nü§ñ –û—Ç–≤–µ—Ç:")
    print(ask(question))