# gradio_chat_ru_pro.py ‚Äî —É–ª—É—á—à–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å –æ—á–∏—Å—Ç–∫–æ–π –∏ –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π –∫–æ–¥–∞

import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from pathlib import Path
import re

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ ===
model_path = Path(__file__).resolve().parent
device = "mps" if torch.backends.mps.is_available() else "cpu"

print(f"üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑: {model_path}")
print(f"üíª –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

tokenizer = AutoTokenizer.from_pretrained(str(model_path))
model = AutoModelForCausalLM.from_pretrained(str(model_path)).to(device)
model.eval()


# === –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ–¥—Å–≤–µ—Ç–∫–∞ –∫–æ–¥–∞ ===
def clean_and_format(text):
    # —É–¥–∞–ª—è–µ–º –≤—Å–µ <|prompt|> <|answer|> –∏ –ª–∏—à–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—ã
    text = re.sub(r"<\|prompt\|>|<\|answer\|>", "", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = text.strip()

    # —Ä–∞—Å–ø–æ–∑–Ω–∞—ë–º shell-–∫–æ–º–∞–Ω–¥—ã –∏ –ø–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º
    def highlight_code(match):
        code = match.group(1)
        return f"```bash\n{code.strip()}\n```"

    text = re.sub(r"(sudo .*?)(?=\n|$)", highlight_code, text)
    return text


# === –õ–æ–≥–∏–∫–∞ –¥–∏–∞–ª–æ–≥–∞ ===
def chat(message, history):
    if history is None:
        history = []

    # –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
    prompt = f"<|prompt|> {message}\n<|answer|>"

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=1024).to(device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=250,
        temperature=0.8,
        top_p=0.95,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.convert_tokens_to_ids("<|prompt|>"),
    )

    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)
    reply = clean_and_format(reply.split("<|answer|>")[-1])

    history.append((message, reply))
    return history, history


# === –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ===
with gr.Blocks(theme=gr.themes.Soft(primary_hue="purple", secondary_hue="orange")) as demo:
    gr.Markdown(
        """
        ## ü§ñ DevOps-LLM (ruGPT3small)
        –¢–≤–æ—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è LLM-–º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ 10 000 DevOps-–ø—Ä–æ–º–ø—Ç–∞—Ö.  
        –°–ø—Ä–∞—à–∏–≤–∞–π –ø—Ä–æ Jenkins, Ansible, Helm, Nginx, Groovy, OpenShift –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ.
        """
    )

    chatbot = gr.Chatbot(label="–î–∏–∞–ª–æ–≥", height=500, show_label=False)
    msg = gr.Textbox(label="–°–æ–æ–±—â–µ–Ω–∏–µ", placeholder="–ù–∞–ø–∏—à–∏ –≤–æ–ø—Ä–æ—Å –∏ –Ω–∞–∂–º–∏ Enter...")
    clear = gr.Button("üßπ –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é")

    msg.submit(chat, [msg, chatbot], [chatbot, chatbot])
    clear.click(lambda: None, None, chatbot, queue=False)

demo.queue()
demo.launch()