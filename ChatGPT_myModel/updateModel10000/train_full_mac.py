# train_rugpt3small.py ‚Äî –æ–±—É—á–µ–Ω–∏–µ —Ä—É—Å—Å–∫–æ–π GPT2 (ruGPT3small) –Ω–∞ DevOps –¥–∞—Ç–∞—Å–µ—Ç–µ

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
import torch

# üöÄ –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU M1)
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# üß† –ë–µ—Ä—ë–º —Ä—É—Å—Å–∫—É—é GPT2 –æ—Ç –°–±–µ—Ä–∞
model_id = "sberbank-ai/rugpt3small_based_on_gpt2"

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token  # GPT2 –Ω–µ –∏–º–µ–µ—Ç pad_token
model = AutoModelForCausalLM.from_pretrained(model_id).to(device)

# üìò –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–≤–æ–π –¥–∞—Ç–∞—Å–µ—Ç
dataset = load_dataset("json", data_files="devops_dataset_10000.jsonl")

# üß© –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç
def preprocess(example):
    text = f"<|prompt|> {example['prompt']}\n<|answer|> {example['completion']}"
    return tokenizer(text, truncation=True, padding="max_length", max_length=512)

tokenized = dataset["train"].map(preprocess, remove_columns=dataset["train"].column_names)

# ‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir="./final_model_ru",
    overwrite_output_dir=True,
    per_device_train_batch_size=1,  # —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ M1
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    save_strategy="epoch",
    logging_dir="./logs_ru",
    logging_steps=10,
    fp16=False,
    report_to="none"
)

# üßë‚Äçüè´ Trainer ‚Äî –æ—Å–Ω–æ–≤–∞ –æ–±—É—á–µ–Ω–∏—è
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
)

# üöÄ –ó–∞–ø—É—Å–∫
trainer.train()

# üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é –º–æ–¥–µ–ª—å
trainer.save_model("./final_model_ru")
tokenizer.save_pretrained("./final_model_ru")
print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./final_model_ru")