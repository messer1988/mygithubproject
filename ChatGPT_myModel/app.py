import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# –ò–º—è –º–æ–¥–µ–ª–∏ –Ω–∞ Hugging Face
model_id = "PythonDevops/devops-llm"

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# –§—É–Ω–∫—Ü–∏—è –æ–±—â–µ–Ω–∏—è —Å –º–æ–¥–µ–ª—å—é
def chat_with_model(message, history=[]):
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥
    inputs = tokenizer(message, return_tensors="pt")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            pad_token_id=tokenizer.eos_token_id
        )

    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä –≤–æ–ø—Ä–æ—Å–∞ –≤ –Ω–∞—á–∞–ª–µ
    if full_output.startswith(message):
        reply = full_output[len(message):].strip()
    else:
        reply = full_output.strip()

    return reply

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio
chat_ui = gr.Interface(
    fn=chat_with_model,
    inputs=gr.Textbox(lines=2, placeholder="–ó–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å –ø–æ DevOps..."),
    outputs="text",
    title="üí¨ DevOps Chat ‚Äî —Ç–≤–æ—è –º–æ–¥–µ–ª—å",
    description="–°–ø—Ä–∞—à–∏–≤–∞–π –æ Jenkins, Ansible, Linux, Docker, Go –∏ —Ç.–¥.",
    theme="default"
)

# –ó–∞–ø—É—Å–∫
if __name__ == "__main__":
    chat_ui.launch()