import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "PythonDevops/devops-llm"

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# –§—É–Ω–∫—Ü–∏—è –æ–±—â–µ–Ω–∏—è —Å –º–æ–¥–µ–ª—å—é
def chat_with_model(message, history=[]):
    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ prompt (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    prompt = message
    inputs = tokenizer(prompt, return_tensors="pt")

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            pad_token_id=tokenizer.eos_token_id
        )

    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return reply

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio
chat_ui = gr.Interface(
    fn=chat_with_model,
    inputs="text",
    outputs="text",
    title="üí¨ DevOps LLM Chat",
    description="–ó–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å –ø–æ Jenkins, Ansible, Linux –∏ —Ç.–¥.",
)

# –ó–∞–ø—É—Å–∫
if __name__ == "__main__":
    chat_ui.launch()